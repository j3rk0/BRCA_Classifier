db <- dataset[dataset$Subtype=="LumB",]
dh <- dataset[dataset$Subtype=="Her2",]
ds <- dataset[dataset$Subtype=="Basal",]
la<-length(da[,1])
lb<-length(db[,1])
lh<-length(dh[,1])
ls<-length(ds[,1])
mean <- (la+lb+lh+ls)/4
if(la > mean) da <- da[sample(1:la,mean),]
if(lb > mean) db <- db[sample(1:lb,mean),]
if(lh > mean) dh <- dh[sample(1:lh,mean),]
if(ls > mean) ds <- ds[sample(1:ls,mean),]
rbind(da,db,dh,ds)
}
balanced_data <- cut_dataset(dataset)
class_numerosity = data.frame(table( balanced_data$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
balanced2 <- cut_dataset(balanced_data)
class_numerosity = data.frame(table( balanced2$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
# seem good
dataset <- balanced2
remove(balanced_data,balanced2,class_numerosity)
model_tuning <- boost_tree(mode="classification",
mtry=tune(),trees = tune(),
min_n=tune(),tree_depth=tune(),
learn_rate=tune(),loss_reduction=tune(),
sample_size=tune()) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
tune_grid <- grid_regular(tree_depth(), trees(c(10,1000)),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dataset_balanced),
learn_rate(),
size = 30)
tuning_result <- workflow() %>% add_formula(Subtype ~ .)  %>% add_model(model_tuning) %>% tune_grid(resamples = vfold_cv(dataset_balanced, 3),grid = tune_grid)
best_parameters <- tuning_result %>% show_best("roc_auc")
model_tuning <- boost_tree(mode="classification",
mtry=tune(),trees = tune(),
min_n=tune(),tree_depth=tune(),
learn_rate=tune(),loss_reduction=tune(),
sample_size=tune()) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
tune_grid <- grid_regular(tree_depth(), trees(c(10,1000)),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dataset_balanced),
learn_rate(),
size = 30)
tune_grid <- grid_regular(tree_depth(), trees(c(10,1000)),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dataset),
learn_rate(),
size = 30)
tune_grid <- grid_regular(tree_depth(), trees(c(10,1000)),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dataset),
learn_rate(),
levels = 30)
tune_grid <- grid_regular(tree_depth(), trees(c(10,1000)),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dataset),
learn_rate())
tuning_result <- workflow() %>% add_formula(Subtype ~ .)  %>% add_model(model_tuning) %>% tune_grid(resamples = vfold_cv(dataset, 3),grid = tune_grid)
install.packages("xgboost")
library(xgboost)
tuning_result <- workflow() %>% add_formula(Subtype ~ .)  %>% add_model(model_tuning) %>% tune_grid(resamples = vfold_cv(dataset, 3),grid = tune_grid)
source("new/library/utils.R")
library(ggplot2)
library(tidymodels)
library(xgboost)
set.seed(12345)
########################DATA LOADING
dataset <- NA
if(!file.exists("new/data/cancer-subtype-seqdata.csv")){ # load data from tcga
source("new/library/tcga-brca.R")
experiment <- load_brca_experiment_data()
clinic_data <- as.data.frame(get_brca_clinic_data(experiment))
gene_expr <- get_brca_genes_count(experiment)
Subtype <-clinic_data$paper_BRCA_Subtype_PAM50
ds<- cbind(Subtype,gene_expr)
ds <- na.omit(ds)
ds <- ds[ds$Subtype!="Normal",]
write.csv(ds,"new/data/cancer-subtype-seqdata.csv")
}
############################### PREPROCESSING
if(!file.exists("new/data/brca_tcga_pam50.csv")) {  # normalize data and get gene  signature
dataset <- read.csv("output/cancer-subtype-seqdata.csv",row.names = 1)
Subtype <- dataset$Subtype
matrix <- dataset[-1]
library(edgeR)
source("library/pam50.R")
source("library/geneOperation.R")
matrix <- t(matrix)
matrix<- cpm(calcNormFactors(DGEList(counts=matrix),normalized.lib.sizes = T))
matrix<-log10(matrix+1)
matrix <- t(matrix)
dataset <- cbind(Subtype,matrix[get_pam50_ids()])
write.csv(dataset,"new/output/brca_tcga_pam50.csv")
}else{
dataset <- read.csv("new/data/brca_tcga_pam50.csv",row.names = 1, stringsAsFactors = T)
}
class_numerosity = data.frame(table(dataset$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
# too unbalanced!!
cut_dataset <- function(dataset){
da <- dataset[dataset$Subtype=="LumA",]
db <- dataset[dataset$Subtype=="LumB",]
dh <- dataset[dataset$Subtype=="Her2",]
ds <- dataset[dataset$Subtype=="Basal",]
la<-length(da[,1])
lb<-length(db[,1])
lh<-length(dh[,1])
ls<-length(ds[,1])
mean <- (la+lb+lh+ls)/4
if(la > mean) da <- da[sample(1:la,mean),]
if(lb > mean) db <- db[sample(1:lb,mean),]
if(lh > mean) dh <- dh[sample(1:lh,mean),]
if(ls > mean) ds <- ds[sample(1:ls,mean),]
rbind(da,db,dh,ds)
}
balanced_data <- cut_dataset(dataset)
class_numerosity = data.frame(table( balanced_data$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
# again?
balanced2 <- cut_dataset(balanced_data)
class_numerosity = data.frame(table( balanced2$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
# seem good
dataset <- balanced2
remove(balanced_data,balanced2,class_numerosity)
########################## HYPERPARAMETER TUNING
model_tuning <- boost_tree(mode="classification",
mtry=tune(),trees = tune(),
min_n=tune(),tree_depth=tune(),
learn_rate=tune(),loss_reduction=tune(),
sample_size=tune()) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
tune_grid <- grid_regular(tree_depth(), trees(c(10,1000)),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dataset),
learn_rate())
tuning_result <- workflow() %>% add_formula(Subtype ~ .)  %>% add_model(model_tuning) %>% tune_grid(resamples = vfold_cv(dataset, 3),grid = tune_grid)
best_parameters <- tuning_result %>% show_best("roc_auc")
View(best_parameters)
model_spec <-  boost_tree(mode="classification",
mtry= best_parameters$mtry[1], trees= best_parameters$trees[1],
min_n= best_parameters$min_n[1], tree_depth= best_parameters$tree_depth[1],
learn_rate= best_parameters$learn_rate[1], loss_reduction= best_parameters$loss_reduction[1],
sample_size= best_parameters$sample_size[1]) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
model <- model_spec %>% fit(Subtype ~ . , dataset)
model <- model_spec %>% fit(Subtype ~ . , dataset)
split <- initial_split(dataset_balanced,4/5)
split <- initial_split(dataset,4/5)
model_eval <- model_spec %>% fit(Subtype ~ . , training(split))
prediction <- predict(model,testing(split))
result <- cbind(predicted=prediction,actual=testing(split)$Subtype)
conf_mat <- conf_mat(result,truth=actual,estimate = .pred_class)
conf_mat <- conf_mat$table
conf_mat
model <- model_spec %>% fit(Subtype ~ . , dataset)
saveRDS(model,"output/model_xgboost.rds")
source("library/utils.R")
library(ggplot2)
library(tidymodels)
library(xgboost)
set.seed(12345)
########################DATA LOADING
dataset <- NA
if(!file.exists("data/cancer-subtype-seqdata.csv")){ # load data from tcga
source("library/tcga-brca.R")
experiment <- load_brca_experiment_data()
clinic_data <- as.data.frame(get_brca_clinic_data(experiment))
gene_expr <- get_brca_genes_count(experiment)
Subtype <-clinic_data$paper_BRCA_Subtype_PAM50
ds<- cbind(Subtype,gene_expr)
ds <- na.omit(ds)
ds <- ds[ds$Subtype!="Normal",]
write.csv(ds,"data/cancer-subtype-seqdata.csv")
}
if(!file.exists("data/brca_tcga_pam50.csv")) {  # normalize data and get gene  signature
dataset <- read.csv("output/cancer-subtype-seqdata.csv",row.names = 1)
Subtype <- dataset$Subtype
matrix <- dataset[-1]
library(edgeR)
source("library/pam50.R")
source("library/geneOperation.R")
matrix <- t(matrix)
matrix<- cpm(calcNormFactors(DGEList(counts=matrix),normalized.lib.sizes = T))
matrix<-log10(matrix+1)
matrix <- t(matrix)
dataset <- cbind(Subtype,matrix[get_pam50_ids()])
write.csv(dataset,"output/brca_tcga_pam50.csv")
}else{
dataset <- read.csv("data/brca_tcga_pam50.csv",row.names = 1, stringsAsFactors = T)
}
if(!file.exists("data/brca_tcga_pam50.csv")) {  # normalize data and get gene  signature
dataset <- read.csv("output/cancer-subtype-seqdata.csv",row.names = 1)
Subtype <- dataset$Subtype
matrix <- dataset[-1]
library(edgeR)
source("library/pam50.R")
source("library/geneOperation.R")
matrix <- t(matrix)
matrix<- cpm(calcNormFactors(DGEList(counts=matrix),normalized.lib.sizes = T))
matrix<-log10(matrix+1)
matrix <- t(matrix)
dataset <- cbind(Subtype,matrix[get_pam50_ids()])
write.csv(dataset,"output/brca_tcga_pam50.csv")
}else{
dataset <- read.csv("data/brca_tcga_pam50.csv",row.names = 1, stringsAsFactors = T)
}
class_numerosity = data.frame(table(dataset$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
cut_dataset <- function(dataset){
da <- dataset[dataset$Subtype=="LumA",]
db <- dataset[dataset$Subtype=="LumB",]
dh <- dataset[dataset$Subtype=="Her2",]
ds <- dataset[dataset$Subtype=="Basal",]
la<-length(da[,1])
lb<-length(db[,1])
lh<-length(dh[,1])
ls<-length(ds[,1])
mean <- (la+lb+lh+ls)/4
if(la > mean) da <- da[sample(1:la,mean),]
if(lb > mean) db <- db[sample(1:lb,mean),]
if(lh > mean) dh <- dh[sample(1:lh,mean),]
if(ls > mean) ds <- ds[sample(1:ls,mean),]
rbind(da,db,dh,ds)
}
balanced_data <- cut_dataset(dataset)
class_numerosity = data.frame(table( balanced_data$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
balanced2 <- cut_dataset(balanced_data)
class_numerosity = data.frame(table( balanced2$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
dataset <- balanced2
remove(balanced_data,balanced2,class_numerosity)
best_parameters <- NA
if(!file.exists("result/best_parameters.csv")){
model_tuning <- boost_tree(mode="classification",
mtry=tune(),trees = tune(),
min_n=tune(),tree_depth=tune(),
learn_rate=tune(),loss_reduction=tune(),
sample_size=tune()) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
tune_grid <- grid_regular(tree_depth(), trees(c(10,1000)),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dataset),
learn_rate())
tuning_result <- workflow() %>% add_formula(Subtype ~ .)  %>% add_model(model_tuning) %>% tune_grid(resamples = vfold_cv(dataset, 3),grid = tune_grid)
best_parameters <- tuning_result %>% show_best("roc_auc")
write.csv(best_parameters,"result/best_parameters.csv")
}else{
best_parameters = read.csv("result/best_parameters.csv")
}
model_spec <-  boost_tree(mode="classification",
mtry= best_parameters$mtry[1], trees= best_parameters$trees[1],
min_n= best_parameters$min_n[1], tree_depth= best_parameters$tree_depth[1],
learn_rate= best_parameters$learn_rate[1], loss_reduction= best_parameters$loss_reduction[1],
sample_size= best_parameters$sample_size[1]) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
split <- initial_split(dataset,4/5)
model_eval <- model_spec %>% fit(Subtype ~ . , training(split))
split <- initial_split(dataset,4/5)
model_eval <- model_spec %>% fit(Subtype ~ . , training(split))
result <- cbind(predicted=prediction,actual=testing(split)$Subtype)
prediction <- predict(model,testing(split))
prediction <- predict(model_eval,testing(split))
result <- cbind(predicted=prediction,actual=testing(split)$Subtype)
conf_mat <- conf_mat(result,truth=actual,estimate = .pred_class)
conf_mat <- conf_mat$table
t <- data.frame(conf_mat)
View(t)
View(best_parameters)
if(!file.exists("result/best_parameters.csv")){
model_tuning <- boost_tree(mode="classification",
mtry=tune(),trees = tune(),
min_n=tune(),tree_depth=tune(),
learn_rate=tune(),loss_reduction=tune(),
sample_size=tune()) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
tune_grid <- grid_regular(tree_depth(), trees(c(10,1000)),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dataset),
learn_rate())
tuning_result <- workflow() %>% add_formula(Subtype ~ .)  %>% add_model(model_tuning) %>% tune_grid(resamples = vfold_cv(dataset, 3),grid = tune_grid)
best_parameters <- tuning_result %>% show_best("roc_auc")
write.csv(best_parameters,"result/best_parameters.csv")
}else{
best_parameters = read.csv("result/best_parameters.csv")
}
split <- initial_split(dataset,4/5)
model_eval <- model_spec %>% fit(Subtype ~ . , training(split))
prediction <- predict(model_eval,testing(split))
result <- cbind(predicted=prediction,actual=testing(split)$Subtype)
conf_mat <- conf_mat(result,truth=actual,estimate = .pred_class)
conf_mat <- conf_mat$table
view(conf_mat)
conf_mat
View(best_parameters)
model_spec <-  boost_tree(mode="classification",
mtry= best_parameters$mtry[3], trees= best_parameters$trees[3],
min_n= best_parameters$min_n[3], tree_depth= best_parameters$tree_depth[3],
learn_rate= best_parameters$learn_rate[3], loss_reduction= best_parameters$loss_reduction[3],
sample_size= best_parameters$sample_size[3]) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
split <- initial_split(dataset,4/5)
model_eval <- model_spec %>% fit(Subtype ~ . , training(split))
prediction <- predict(model_eval,testing(split))
result <- cbind(predicted=prediction,actual=testing(split)$Subtype)
conf_mat <- conf_mat(result,truth=actual,estimate = .pred_class)
conf_mat <- conf_mat$table
conf_mat
1
split <- initial_split(dataset,4/5)
model_spec <-  boost_tree(mode="classification",
mtry= best_parameters$mtry[1], trees= best_parameters$trees[1],
min_n= best_parameters$min_n[1], tree_depth= best_parameters$tree_depth[1],
learn_rate= best_parameters$learn_rate[1], loss_reduction= best_parameters$loss_reduction[1],
sample_size= best_parameters$sample_size[1]) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
split <- initial_split(dataset,4/5)
model_eval <- model_spec %>% fit(Subtype ~ . , training(split))
prediction <- predict(model_eval,testing(split))
result <- cbind(predicted=prediction,actual=testing(split)$Subtype)
conf_mat <- conf_mat(result,truth=actual,estimate = .pred_class)
conf_mat <- conf_mat$table
conf_mat
ggplot() +
# good!!
############### TRAINING
model <- model_spec %>% fit(Subtype ~ . , dataset)
model <- model_spec %>% fit(Subtype ~ . , dataset)
saveRDS(model,"output/model_xgboost.rds")
saveRDS(model,"result/model_xgboost.rds")
source("library/utils.R")
library(ggplot2)
library(tidymodels)
library(xgboost)
set.seed(12345)
########################DATA LOADING
dataset <- NA
if(!file.exists("data/cancer-subtype-seqdata.csv")){ # load data from tcga
source("library/tcga-brca.R")
experiment <- load_brca_experiment_data()
clinic_data <- as.data.frame(get_brca_clinic_data(experiment))
gene_expr <- get_brca_genes_count(experiment)
Subtype <-clinic_data$paper_BRCA_Subtype_PAM50
ds<- cbind(Subtype,gene_expr)
ds <- na.omit(ds)
ds <- ds[ds$Subtype!="Normal",]
write.csv(ds,"data/cancer-subtype-seqdata.csv")
}
############################### PREPROCESSING
if(!file.exists("data/brca_tcga_pam50.csv")) {  # normalize data and get gene  signature
dataset <- read.csv("output/cancer-subtype-seqdata.csv",row.names = 1)
Subtype <- dataset$Subtype
matrix <- dataset[-1]
library(edgeR)
source("library/pam50.R")
source("library/geneOperation.R")
matrix <- t(matrix)
matrix<- cpm(calcNormFactors(DGEList(counts=matrix),normalized.lib.sizes = T))
matrix<-log10(matrix+1)
matrix <- t(matrix)
dataset <- cbind(Subtype,matrix[get_pam50_ids()])
write.csv(dataset,"output/brca_tcga_pam50.csv")
}else{
dataset <- read.csv("data/brca_tcga_pam50.csv",row.names = 1, stringsAsFactors = T)
}
class_numerosity = data.frame(table(dataset$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
# too unbalanced!!
cut_dataset <- function(dataset){
da <- dataset[dataset$Subtype=="LumA",]
db <- dataset[dataset$Subtype=="LumB",]
dh <- dataset[dataset$Subtype=="Her2",]
ds <- dataset[dataset$Subtype=="Basal",]
la<-length(da[,1])
lb<-length(db[,1])
lh<-length(dh[,1])
ls<-length(ds[,1])
mean <- (la+lb+lh+ls)/4
if(la > mean) da <- da[sample(1:la,mean),]
if(lb > mean) db <- db[sample(1:lb,mean),]
if(lh > mean) dh <- dh[sample(1:lh,mean),]
if(ls > mean) ds <- ds[sample(1:ls,mean),]
rbind(da,db,dh,ds)
}
balanced_data <- cut_dataset(dataset)
class_numerosity = data.frame(table( balanced_data$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
# again?
balanced2 <- cut_dataset(balanced_data)
class_numerosity = data.frame(table( balanced2$Subtype))
ggplot(data=class_numerosity, aes(x=Var1,y=Freq)) + geom_bar(stat="identity")
# seem good
dataset <- balanced2
remove(balanced_data,balanced2,class_numerosity)
########################## HYPERPARAMETER TUNING
best_parameters <- NA
if(!file.exists("result/best_parameters.csv")){
model_tuning <- boost_tree(mode="classification",
mtry=tune(),trees = tune(),
min_n=tune(),tree_depth=tune(),
learn_rate=tune(),loss_reduction=tune(),
sample_size=tune()) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
tune_grid <- grid_regular(tree_depth(), trees(c(10,1000)),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dataset),
learn_rate())
tuning_result <- workflow() %>% add_formula(Subtype ~ .)  %>% add_model(model_tuning) %>% tune_grid(resamples = vfold_cv(dataset, 3),grid = tune_grid)
best_parameters <- tuning_result %>% show_best("roc_auc")
write.csv(best_parameters,"result/best_parameters.csv")
}else{
best_parameters = read.csv("result/best_parameters.csv")
}
model_spec <-  boost_tree(mode="classification",
mtry= best_parameters$mtry[1], trees= best_parameters$trees[1],
min_n= best_parameters$min_n[1], tree_depth= best_parameters$tree_depth[1],
learn_rate= best_parameters$learn_rate[1], loss_reduction= best_parameters$loss_reduction[1],
sample_size= best_parameters$sample_size[1]) %>% set_engine("xgboost", objective = "multi:softprob", num_class=4)
################# VALIDATION
split <- initial_split(dataset,4/5)
model_eval <- model_spec %>% fit(Subtype ~ . , training(split))
prediction <- predict(model_eval,testing(split))
result <- cbind(predicted=prediction,actual=testing(split)$Subtype)
conf_mat <- conf_mat(result,truth=actual,estimate = .pred_class)
conf_mat <- conf_mat$table
conf_mat
model <- model_spec %>% fit(Subtype ~ . , dataset)
saveRDS(model,"result/model_xgboost.rds")
source("library/utils.R")
library(Matrix)
library(ggplot2)
library(tidymodels)
library(xgboost)
library(edgeR)
set.seed(12345)
#HUMAN BREAST CANCER FROM 10XGENOMICS VISIUM SPATIAL PROTOCOLS
spatial_expression <- as.matrix(readMM("data/spatial/matrix.mtx"))
#NORMALIZATION
spatial_expression<- cpm(calcNormFactors(DGEList(counts=spatial_expression),normalized.lib.sizes = T))
spatial_expression<-log10(spatial_expression+1)
spatial_expression <- t(spatial_expression)
features <- read.csv("data/spatial/features.tsv", sep="\t",header = F)
barcodes <- read.csv("data/spatial/barcodes.tsv",sep="\t",header =F )
indexes <- read.csv("data/spatial/cells_indexes.csv")
indexes <- indexes[match(barcodes$V1,indexes$barcode),]
DATA <- data.frame(spatial_expression)
DATA <- cbind(indexes,DATA)
#removing barcodes with few reads
percentage <- (ncol(DATA)/100)*(95)
DATA = DATA[rowSums(DATA[,4:36604]==0)<=percentage,]
remove(percentage,barcodes,featires,indexes,spatial_expression)
remove(percentage,barcodes,features,indexes,spatial_expression)
View(DATA)
features <- read.csv("data/spatial/features.tsv", sep="\t",header = F)
colnames(DATA)[4:36604] <- features$V1
View(DATA)
remove(percentage,barcodes,features,indexes,spatial_expression)
barcodes <- DATA[1:3]
reads <- DATA[4:36604]
model <- readRDS("result/model_xgboost.rds")
remove(model,barcodes,reads)
pam50 <- read.csv("resources/pam50.csv")
View(pam50)
pam50_data <- DATA[pam50$ens]
remove(DATA,pam50,percentage,barcodes,features,indexes,spatial_expression)
View(pam50_data)
library(Matrix)
library(ggplot2)
library(tidymodels)
library(xgboost)
library(edgeR)
set.seed(12345)
#HUMAN BREAST CANCER FROM 10XGENOMICS VISIUM SPATIAL PROTOCOLS
spatial_expression <- as.matrix(readMM("data/spatial/matrix.mtx"))
#NORMALIZATION
spatial_expression<- cpm(calcNormFactors(DGEList(counts=spatial_expression),normalized.lib.sizes = T))
spatial_expression<-log10(spatial_expression+1)
spatial_expression <- t(spatial_expression)
features <- read.csv("data/spatial/features.tsv", sep="\t",header = F)
barcodes <- read.csv("data/spatial/barcodes.tsv",sep="\t",header =F )
indexes <- read.csv("data/spatial/cells_indexes.csv")
indexes <- indexes[match(barcodes$V1,indexes$barcode),]
DATA <- data.frame(spatial_expression)
DATA <- cbind(indexes,DATA)
#removing barcodes with few reads
percentage <- (ncol(DATA)/100)*(95)
DATA = DATA[rowSums(DATA[,4:36604]==0)<=percentage,]
colnames(DATA)[4:36604] <- features$V1
barcodes <- DATA[1:3]
pam50 <- read.csv("resources/pam50.csv")
pam50_reads <- DATA[pam50$ens]
View(barcodes)
remove(DATA,pam50,percentage,features,indexes,spatial_expression)
model <- readRDS("result/model_xgboost.rds")
res <- predict(model,pam50_reads)
View(res)
res <- cbind(barcodes,res)
View(res)
ggplot(data=res,aes(x=array_col,y=array_row, colour=.pred_class))+geom_point()
